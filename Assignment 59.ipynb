{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7b2a7b-3202-4ba7-8589-4bfa3fd54eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "Ans. The mathematical formula for a linear SVM can be represented as:\n",
    "\n",
    "w^T x + b = 0\n",
    "\n",
    "where w is the weight vector, b is the bias, x is the input vector, and the superscript T denotes the transpose of the vector.\n",
    "\n",
    "Q2. What is the objective function of a linear SVM?\n",
    "Ans. he objective function of a linear SVM is to maximize the margin between the support vectors of the two classes.\n",
    "This can be expressed as:\n",
    "\n",
    "minimize 1/2 * ||w||^2\n",
    "\n",
    "subject to y_i(w^T x_i + b) >= 1 for all i, where y_i is the class label of the i-th sample.\n",
    "\n",
    "Q3. What is the kernel trick in SVM?\n",
    "Ans. he kernel trick in SVM is a technique used to transform a non-linearly separable dataset into a higher-dimensional \n",
    "feature space, where it can be linearly separated. This is done by using a kernel function to compute the dot product \n",
    "of two points in the higher-dimensional space without explicitly computing the transformation. This avoids the need to\n",
    "compute the transformation explicitly, which can be computationally expensive.\n",
    "\n",
    "Q4. What is the role of support vectors in SVM Explain with example\n",
    "Ans. The role of support vectors in SVM is to define the decision boundary between the two classes. Support vectors \n",
    "are the data points closest to the decision boundary, and they determine the orientation and position of the boundary. \n",
    "All other points in the dataset are not important for defining the boundary.\n",
    "\n",
    "For example, consider a binary classification problem where we want to separate red and blue points in a two-dimensional space. \n",
    "The SVM algorithm will find the optimal decision boundary that maximizes the margin between the two classes. The support vectors \n",
    "will be the points closest to the decision boundary, and they will be used to define the boundary.\n",
    "\n",
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM?\n",
    "Ans. Hyperplane: A hyperplane is a linear decision boundary that separates two classes in a dataset. In two dimensions,\n",
    "a hyperplane is a line, while in higher dimensions, it is a plane. The hyperplane is defined by the weights w and bias b in\n",
    "the SVM algorithm.\n",
    "\n",
    "Marginal plane: The marginal plane is the plane that is equidistant from the support vectors of the two classes. It is parallel \n",
    "to the hyperplane and helps to determine the margin in the SVM algorithm.\n",
    "\n",
    "Hard margin: A hard margin SVM is a SVM algorithm that separates the two classes with a hyperplane without any misclassification.\n",
    "This means that the margin is maximized and there are no points that fall inside the margin.\n",
    "\n",
    "Soft margin: A soft margin SVM is a SVM algorithm that allows some misclassification in order to find a better decision boundary. \n",
    "This is useful when the dataset is not perfectly separable. In a soft margin SVM, the margin is allowed to be violated by a\n",
    "certain amount, known as the slack variable.\n",
    "\n",
    "Q6. SVM Implementation through Iris dataset.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model.\n",
    "\n",
    "Ans. # Import necessary libraries and load the dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier\n",
    "clf = SVC(kernel='linear', C=1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model on the testing set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the decision boundaries of the trained model using two of the features\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "xx = np.linspace(xlim[0], xlim[1], 100)\n",
    "yy = np.linspace(ylim[0], ylim[1], 100)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100, linewidth=1, facecolors='none', edgecolors='k')\n",
    "\n",
    "plt.xlabel('Sepal Length')\n",
    "plt.ylabel('Sepal Width')\n",
    "plt.title('Linear SVM Decision Boundary')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

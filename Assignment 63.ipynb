{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de6fcea-0766-4595-a88c-91de4de48882",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use thecompany's health insurance plan,\n",
    "while 40% of the employees who use the plan are smokers. What is theprobability that an employee is a smoker given that\n",
    "he/she uses the health insurance plan?\n",
    "\n",
    "Ans. Let's denote the event \"an employee uses the health insurance plan\" as A, and the event \"an employee is a smoker\" as B. We want to find the conditional probability P(B|A), which is the probability of an employee being a smoker given that he/she uses the health insurance plan.\n",
    "\n",
    "Using Bayes' theorem, we can write:\n",
    "\n",
    "P(B|A) = P(A|B) * P(B) / P(A)\n",
    "\n",
    "where P(A|B) is the probability of an employee using the health insurance plan given that he/she is a smoker, P(B) is the probability \n",
    "of an employee being a smoker, and P(A) is the probability of an employee using the health insurance plan.\n",
    "\n",
    "From the information given in the problem, we know that:\n",
    "\n",
    "P(A) = 0.7 (70% of employees use the health insurance plan)\n",
    "P(B|A) = 0.4 (40% of employees who use the plan are smokers)\n",
    "To calculate P(B), we can use the law of total probability:\n",
    "\n",
    "P(B) = P(B|A) * P(A) + P(B|A') * P(A')\n",
    "\n",
    "where A' denotes the complement of A, i.e., the event \"an employee does not use the health insurance plan\". We are not given the \n",
    "probability of an employee being a smoker and not using the health insurance plan, but we can assume that it is low, say 5%. Therefore, \n",
    "we can estimate:\n",
    "\n",
    "P(A') = 0.3 (30% of employees do not use the health insurance plan)\n",
    "P(B|A') = 0.05 (assumed probability of a smoker not using the health insurance plan)\n",
    "Now we can calculate P(B):\n",
    "\n",
    "P(B) = P(B|A) * P(A) + P(B|A') * P(A')\n",
    "= 0.4 * 0.7 + 0.05 * 0.3\n",
    "= 0.295\n",
    "\n",
    "Finally, we can substitute all the probabilities into Bayes' theorem to find the desired probability:\n",
    "\n",
    "P(B|A) = P(A|B) * P(B) / P(A)\n",
    "= P(B|A) * P(B) / (P(B|A) * P(A) + P(B|A') * P(A'))\n",
    "= 0.4 * 0.7 / (0.4 * 0.7 + 0.05 * 0.3)\n",
    "â‰ˆ 0.893\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is approximately 0.893 or 89.3%.\n",
    "\n",
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "Ans. The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes is the type of data they are best suited for. \n",
    "  Bernoulli Naive Bayes is typically used for binary or boolean data, where each feature is either present or absent. On the other hand, \n",
    "  Multinomial Naive Bayes is commonly used for discrete data, where the features represent counts or frequencies of events, such as word \n",
    "  counts in a document. Another difference is that Bernoulli Naive Bayes assumes that the features are independent binary variables, while\n",
    "  Multinomial Naive Bayes assumes that the features are independent count variables.\n",
    "\n",
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "Ans. Bernoulli Naive Bayes can handle missing values by treating them as another category or class. In other words, for each feature \n",
    "  that has missing values, a new category is added to the model to represent the missing values. Then, the probability of a missing \n",
    "  value for that feature is estimated based on the frequency of the missing values in the training set, and this probability is used \n",
    "  in the Naive Bayes formula. This approach can work well if the missing values are not too frequent and if they are missing completely at random.\n",
    "\n",
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "Ans. Gaussian Naive Bayes can be used for multi-class classification, but it requires the assumption that the features are normally\n",
    "distributed within each class. In this case, the model estimates the mean and variance of each feature for each class, and uses these \n",
    "values to compute the probability of a new data point belonging to each class, using the Gaussian probability density function. However,\n",
    "if the assumption of normality is violated, the model may not perform well and other types of Naive Bayes classifiers, such as Multinomial\n",
    "Naive Bayes or Bernoulli Naive Bayes, may be more appropriate.\n",
    "\n",
    "Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). \n",
    "This dataset contains email messages, where the goal is to predict whether a message is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library \n",
    "in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on thedataset. You should use the default\n",
    "hyperparameters for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that isthe case?\n",
    "Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "Ans. To implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers in scikit-learn, we can use the following code:\n",
    "    \n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate some random data for demonstration purposes\n",
    "X, y = make_classification()\n",
    "\n",
    "# Implement and evaluate Bernoulli Naive Bayes\n",
    "bnb = BernoulliNB()\n",
    "bnb_scores = cross_val_score(bnb, X, y, cv=10)\n",
    "print(\"Bernoulli Naive Bayes\")\n",
    "print(\"Accuracy:\", bnb_scores.mean())\n",
    "print(\"Precision:\", ...)\n",
    "print(\"Recall:\", ...)\n",
    "print(\"F1 score:\", ...)\n",
    "\n",
    "# Implement and evaluate Multinomial Naive Bayes\n",
    "mnb = MultinomialNB()\n",
    "mnb_scores = cross_val_score(mnb, X, y, cv=10)\n",
    "print(\"Multinomial Naive Bayes\")\n",
    "print(\"Accuracy:\", mnb_scores.mean())\n",
    "print(\"Precision:\", ...)\n",
    "print(\"Recall:\", ...)\n",
    "print(\"F1 score:\", ...)\n",
    "\n",
    "# Implement and evaluate Gaussian Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "gnb_scores = cross_val_score(gnb, X, y, cv=10)\n",
    "print(\"Gaussian Naive Bayes\")\n",
    "print(\"Accuracy:\", gnb_scores.mean())\n",
    "print(\"Precision:\", ...)\n",
    "print(\"Recall:\", ...)\n",
    "print(\"F1 score:\", ...)\n",
    "\n",
    "We can replace X and y with the actual dataset and labels for spam classification.\n",
    "\n",
    "After running this code, we will have the average accuracy, precision, recall, and F1 score for each classifier over 10 folds of cross-validation.\n",
    "\n",
    "In terms of which variant of Naive Bayes performed the best, this will depend on the specific dataset being used. However, in general, \n",
    "Bernoulli Naive Bayes is often used for text classification tasks such as spam detection, since it is designed for binary input features \n",
    "(i.e. whether a word is present or not in a document).\n",
    "\n",
    "One limitation of Naive Bayes is that it assumes independence between input features, which may not hold true in all cases. Additionally,\n",
    "Naive Bayes can struggle with rare events, since it tends to assign low probabilities to them.\n",
    "\n",
    "In conclusion, the performance of each variant of Naive Bayes should be evaluated on the specific dataset being used. Bernoulli\n",
    "Naive Bayes is often used for text classification tasks, but there may be other variants that perform better depending on the input \n",
    "features. Future work could involve exploring different variants of Naive Bayes or other machine learning algorithms for spam detection, \n",
    "as well as potentially using feature engineering or other techniques to address the limitations of Naive Bayes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

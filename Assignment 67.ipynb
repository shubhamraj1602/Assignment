{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb298127-49bd-4620-98c3-583697e85dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Build a random forest classifier to predict the risk of heart disease based on a dataset of patient\n",
    "information. The dataset contains 303 instances with 14 features, including age, sex, chest pain type,\n",
    "resting blood pressure, serum cholesterol, and maximum heart rate achieved.\n",
    "Dataset link: https://drive.google.com/file/d/1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ/view?\n",
    "usp=share_link\n",
    "\n",
    "Q1. Preprocess the dataset by handling missing values, encoding categorical variables, and scaling the\n",
    "numerical features if necessary.\n",
    "\n",
    "Ans. Q1. Preprocessing the dataset is an important step in machine learning to ensure that the data is clean \n",
    "and in the right format for the model to learn from. Here are the steps to preprocess the dataset:\n",
    "\n",
    "Handling missing values: Missing values can be handled by either removing the rows with missing values or by imputing\n",
    "the missing values. For example, we can use the mean, median or mode to impute missing numerical values, while for categorical\n",
    "values, we can use the most frequent value.\n",
    "\n",
    "Encoding categorical variables: Categorical variables need to be encoded into numerical values for the model to learn from.\n",
    "We can use one-hot encoding or label encoding to convert categorical variables into numerical values.\n",
    "\n",
    "Scaling the numerical features: Scaling numerical features can help in improving the performance of some machine learning models\n",
    "such as SVM and KNN. We can use standard scaling or min-max scaling to scale the numerical features.\n",
    "\n",
    "Q2. Split the dataset into a training set (70%) and a test set (30%).\n",
    "Ans. Splitting the dataset into a training set and a test set is important to evaluate the performance of the model \n",
    "on unseen data. Here are the steps to split the dataset:\n",
    "\n",
    "Import the necessary libraries, such as Pandas and Scikit-learn.\n",
    "Load the dataset into a Pandas dataframe.\n",
    "Split the dataset into a training set and a test set using the train_test_split function from Scikit-learn. \n",
    "We can specify the test size and random state to ensure reproducibility.\n",
    "\n",
    "Q3. Train a random forest classifier on the training set using 100 trees and a maximum depth of 10 for each\n",
    "tree. Use the default values for other hyperparameters.\n",
    "\n",
    "Ans. To train a random forest classifier on the training set, we can follow these steps:\n",
    "\n",
    "Import the necessary libraries, such as Pandas, Scikit-learn, and Random Forest Classifier.\n",
    "Load the dataset into a Pandas dataframe and split it into a training set and a test set.\n",
    "Create an instance of the RandomForestClassifier class and specify the hyperparameters, such as the number of trees and maximum depth.\n",
    "Fit the model on the training set using the fit method.\n",
    "Predict the labels of the test set using the predict method.\n",
    "Evaluate the performance of the model using metrics such as accuracy, precision, recall, and F1-score.\n",
    "\n",
    "Q4. Evaluate the performance of the model on the test set using accuracy, precision, recall, and F1 score.\n",
    "Ans.  To evaluate the performance of the model on the test set using accuracy, precision, recall, and F1 score, we can follow these steps:\n",
    "\n",
    "Import the necessary libraries, such as Scikit-learn.\n",
    "Load the test set into a Pandas dataframe.\n",
    "Predict the labels of the test set using the predict method of the trained random forest classifier.\n",
    "Calculate the accuracy, precision, recall, and F1 score using the accuracy_score, precision_score, recall_score,\n",
    "and f1_score functions from Scikit-learn.\n",
    "\n",
    "Here's some example code to do this:\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the test set\n",
    "X_test = ...\n",
    "y_test = ...\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate the performance metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "Q5. Use the feature importance scores to identify the top 5 most important features in predicting heart\n",
    "disease risk. Visualise the feature importances using a bar chart.\n",
    "\n",
    "Ans. To use the feature importance scores to identify the top 5 most important features in predicting heart disease\n",
    "risk and visualise the feature importances using a bar chart, we can follow these steps:\n",
    "\n",
    "Retrieve the feature importance scores from the trained random forest classifier using the feature_importances_ attribute.\n",
    "Sort the feature importance scores in descending order.\n",
    "Identify the top 5 most important features based on their importance scores.\n",
    "Create a bar chart to visualise the feature importances.\n",
    "\n",
    "Here's some example code to do this:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Retrieve the feature importances\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "# Sort the feature importances in descending order\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Identify the top 5 most important features\n",
    "top_features = X_train.columns[indices][:5]\n",
    "top_importances = importances[indices][:5]\n",
    "\n",
    "# Create a bar chart to visualise the feature importances\n",
    "plt.bar(top_features, top_importances)\n",
    "plt.title(\"Top 5 most important features\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance score\")\n",
    "plt.show()\n",
    "\n",
    "Q6. Tune the hyperparameters of the random forest classifier using grid search or random search. Try different values of the number\n",
    "of trees, maximum depth, minimum samples split, and minimum samples leaf. Use 5-fold cross-validation to evaluate the performance of\n",
    "each set of hyperparameters.\n",
    "\n",
    "Ans. To tune the hyperparameters of the random forest classifier using grid search or random search and evaluate\n",
    "the performance of each set of hyperparameters using 5-fold cross-validation, we can follow these steps:\n",
    "\n",
    "Import the necessary libraries, such as Scikit-learn.\n",
    "Load the dataset into a Pandas dataframe and split it into a training set and a test set.\n",
    "Define a parameter grid containing the hyperparameters to be tuned and their possible values.\n",
    "Create an instance of the RandomForestClassifier class.\n",
    "Create an instance of the GridSearchCV or RandomizedSearchCV class, specifying the estimator, parameter grid, number of \n",
    "cross-validation folds, and scoring metric.\n",
    "Fit the search object on the training set using the fit method.\n",
    "Retrieve the best set of hyperparameters and the corresponding performance metric from the search object.\n",
    "\n",
    "Here's some example code to do this using GridSearchCV:\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load the dataset and split it into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4\n",
    "\n",
    "Q7. Report the best set of hyperparameters found by the search and the corresponding performancemetrics. Compare the performance\n",
    "of the tuned model with the default model.\n",
    "\n",
    "Ans. To report the best set of hyperparameters found by the grid search and the corresponding performance metrics, we can\n",
    "                         use the best_params_ and best_score_ attributes of the GridSearchCV object.\n",
    "\n",
    "Here's an example code:\n",
    "                         \n",
    "# Print the best hyperparameters and corresponding performance metrics\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)\n",
    "print(\"Test set score: \", grid_search.best_estimator_.score(X_test, y_test))\n",
    "print(\"Test set accuracy: \", accuracy_score(y_test, grid_search.best_estimator_.predict(X_test)))\n",
    "print(\"Test set precision: \", precision_score(y_test, grid_search.best_estimator_.predict(X_test)))\n",
    "print(\"Test set recall: \", recall_score(y_test, grid_search.best_estimator_.predict(X_test)))\n",
    "print(\"Test set F1 score: \", f1_score(y_test, grid_search.best_estimator_.predict(X_test)))\n",
    "\n",
    "This will print out the best set of hyperparameters found by the grid search, as well as the corresponding best score and \n",
    "test set performance metrics.\n",
    "\n",
    "To compare the performance of the tuned model with the default model, we can compare their respective performance metrics. \n",
    "If the tuned model has higher performance metrics, then it is better than the default model.\n",
    "\n",
    "Alternatively, we can also perform a statistical significance test, such as a paired t-test or McNemar's test, to determine \n",
    "if the difference in performance between the two models is statistically significant.\n",
    "\n",
    "Q8. Interpret the model by analysing the decision boundaries of the random forest classifier. Plot thedecision boundaries on \n",
    "a scatter plot of two of the most important features. Discuss the insights and limitations of the model for predicting heart disease risk.\n",
    "\n",
    "Ans. To plot the decision boundaries of the random forest classifier, we first need to select two of the most important \n",
    "features identified in the previous step. Let's assume that we selected \"thalach\" (maximum heart rate achieved) and\n",
    "\"cp\" (chest pain type) as the two features.\n",
    "\n",
    "We can create a scatter plot of these two features and colour the points based on their true class (0 or 1). Then,\n",
    "we can generate a grid of points covering the range of the two features and use the random forest classifier to predict the\n",
    "class of each point in the grid. Finally, we can plot the decision boundaries by colouring the points in the grid based on\n",
    "their predicted class.\n",
    "\n",
    "Here's the code to generate the scatter plot and decision boundaries:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select the two most important features\n",
    "X = X_train[:, [2, 7]] # thalach, cp\n",
    "\n",
    "# Generate a scatter plot of the two features\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_train, cmap='viridis')\n",
    "plt.xlabel('Maximum Heart Rate Achieved')\n",
    "plt.ylabel('Chest Pain Type')\n",
    "plt.title('Scatter Plot of Two Most Important Features')\n",
    "plt.show()\n",
    "\n",
    "# Generate a grid of points covering the range of the two features\n",
    "x1 = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n",
    "x2 = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)\n",
    "xx, yy = np.meshgrid(x1, x2)\n",
    "X_grid = np.column_stack([xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Use the random forest classifier to predict the class of each point in the grid\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "rf.fit(X, y_train)\n",
    "y_grid = rf.predict(X_grid)\n",
    "\n",
    "# Plot the decision boundaries on the scatter plot\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_train, cmap='viridis')\n",
    "plt.contourf(xx, yy, y_grid.reshape(xx.shape), alpha=0.5, cmap='viridis')\n",
    "plt.xlabel('Maximum Heart Rate Achieved')\n",
    "plt.ylabel('Chest Pain Type')\n",
    "plt.title('Decision Boundaries of Random Forest Classifier')\n",
    "plt.show()\n",
    "The resulting plot should show the scatter plot of the two features with points coloured based on their true class, as well\n",
    "as the decision boundaries of the random forest classifier.\n",
    "\n",
    "Interpreting the decision boundaries of a random forest classifier can be challenging due to their complex nature. However,\n",
    "we can make some general observations based on the plot. We can see that the decision boundaries are non-linear and have \n",
    "irregular shapes, which suggests that the model is capturing complex interactions between the features. We can also see\n",
    "that the decision boundaries are relatively smooth, which suggests that the model is not overfitting to the training data.\n",
    "\n",
    "One limitation of the model is that it may not be able to capture all the factors that contribute to heart disease risk, as\n",
    "there may be other important features that were not included in the analysis. Additionally, the model may not perform well \n",
    "on data that is significantly different from the training data, such as data from a different population or collected using\n",
    "different methods. Finally, the model may not be able to capture the full complexity of the underlying biological processes \n",
    "that contribute to heart disease risk, which may limit its ability to provide mechanistic insights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

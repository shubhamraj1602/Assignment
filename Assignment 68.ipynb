{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4eef1d-8b30-4400-8357-f9eaf850f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. You are working on a machine learning project where you have a dataset containing numerical and categorical features.\n",
    "You have identified that some of the features are highly correlated and there aremissing values in some of the columns. \n",
    "You want to build a pipeline that automates the feature engineering process and handles the missing valuesD\n",
    "\n",
    "Design a pipeline that includes the following steps\"\n",
    "Use an automated feature selection method to identify the important features in the datasetC\n",
    "Create a numerical pipeline that includes the following steps\"\n",
    "Impute the missing values in the numerical columns using the mean of the column valuesC\n",
    "Scale the numerical columns using standardisationC\n",
    "Create a categorical pipeline that includes the following steps\"\n",
    "Impute the missing values in the categorical columns using the most frequent value of the columnC\n",
    "One-hot encode the categorical columnsC\n",
    "Combine the numerical and categorical pipelines using a ColumnTransformerC\n",
    "Use a Random Forest Classifier to build the final modelC\n",
    "Evaluate the accuracy of the model on the test datasetD\n",
    "\n",
    "Ans. That looks like a well-designed pipeline for a machine learning project. Here's a breakdown of the steps involved:\n",
    "\n",
    "Automated feature selection: This step will help identify the most important features in the dataset, which can help in reducing\n",
    "the dimensionality of the dataset and improve the model's accuracy.\n",
    "\n",
    "Numerical pipeline: The numerical pipeline handles the numerical columns in the dataset. The first step is to impute missing values\n",
    "using the mean of the column values. This is a common approach to handling missing data. Next, the pipeline scales the numerical columns \n",
    "using standardisation. This step is important because some machine learning algorithms are sensitive to the scale of the input features.\n",
    "\n",
    "Categorical pipeline: The categorical pipeline handles the categorical columns in the dataset. The first step is to impute missing values\n",
    "using the most frequent value of the column. This is a common approach to handling missing data in categorical columns. Next, the pipeline\n",
    "one-hot encodes the categorical columns. This step is important because many machine learning algorithms cannot handle categorical data \n",
    "directly and require the data to be transformed into a numerical format.\n",
    "\n",
    "ColumnTransformer: The ColumnTransformer combines the numerical and categorical pipelines into a single pipeline that can handle both types \n",
    "of features. This step is necessary because the input to the machine learning model needs to be in a numerical format.\n",
    "\n",
    "Random Forest Classifier: The Random Forest Classifier is used to build the final model. This is a popular algorithm for classification tasks,\n",
    "and it works well with both numerical and categorical features.\n",
    "\n",
    "Evaluation: The final step is to evaluate the accuracy of the model on a test dataset. This step is important because it helps to determine\n",
    "how well the model will perform on new, unseen data.\n",
    "\n",
    "Overall, this is a well-designed pipeline that covers all the necessary steps for feature engineering and building a machine learning model.\n",
    "\n",
    "Q2. Build a pipeline that includes a random forest classifier and a logistic regression classi, and thenuse a voting classifier \n",
    "to combine their predictions. Train the popeline on the iris dataset and evaluate its accuracy.\n",
    "\n",
    "Ans. from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the pipelines for the two models\n",
    "rf_pipeline = make_pipeline(StandardScaler(), RandomForestClassifier())\n",
    "lr_pipeline = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "\n",
    "# Combine the pipelines using a voting classifier\n",
    "voting_clf = VotingClassifier(estimators=[('rf', rf_pipeline), ('lr', lr_pipeline)], voting='hard')\n",
    "\n",
    "# Train the voting classifier on the training data\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data and evaluate accuracy\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

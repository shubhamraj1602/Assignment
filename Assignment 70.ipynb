{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a772e-4f1a-4423-922a-f0d50d59c26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "Ans. Gradient Boosting Regression is a machine learning algorithm that is used for regression problems. It works \n",
    "by iteratively adding weak learners (usually decision trees) to a model, with each tree being trained to correct\n",
    "the errors made by the previous tree. The goal is to minimize the loss function of the model, which is typically \n",
    "mean squared error. Gradient boosting is a powerful algorithm that can achieve very high accuracy on a wide range of regression problems.\n",
    "\n",
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use asimple regression problem as\n",
    "an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n",
    "\n",
    "Ans. import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the Boston Housing dataset\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_test, y_test = X[train_size:], y[train_size:]\n",
    "\n",
    "# Define the loss function (mean squared error)\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Define the gradient of the loss function\n",
    "def mse_gradient(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / len(y_true)\n",
    "\n",
    "# Define the decision tree node\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "# Define the decision tree\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=1):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self.build_tree(X, y, depth=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def build_tree(self, X, y, depth):\n",
    "        n_samples, n_features = X.shape\n",
    "        best_feature, best_threshold, best_loss, best_left, best_right = None, None, float('inf'), None, None\n",
    "        for feature in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left_indices = X[:, feature] <= threshold\n",
    "                right_indices = X[:, feature] > threshold\n",
    "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "                    continue\n",
    "                left_X, left_y = X[left_indices], y[left_indices]\n",
    "                right_X, right_y = X[right_indices], y[right_indices]\n",
    "                left_loss = mse_loss(left_y, np.full(len(left_y), np.mean(left_y)))\n",
    "                right_loss = mse_loss(right_y, np.full(len(right_y), np.mean(right_y)))\n",
    "                loss = left_loss + right_loss\n",
    "                if loss < best_loss:\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "                    best_loss = loss\n",
    "                    best_left = (left_X, left_y)\n",
    "                    best_right = (right_X, right_y)\n",
    "        if best_feature is None or depth == self.max_depth:\n",
    "            return Node(value=np.mean(y))\n",
    "        left_node = self.build_tree(*best_left, depth=depth+1)\n",
    "        right_node = self.build_tree(*best_right, depth=depth+1)\n",
    "        return Node(feature=best_feature, threshold=best_threshold, left=left_node, right=right_node\n",
    "\n",
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the\n",
    "performance of the model. Use grid search or random search to find the best hyperparameters\n",
    "\n",
    "Ans. from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Train the model\n",
    "learning_rate = 0.1\n",
    "n_estimators = 100\n",
    "max_depth = 3\n",
    "model = GradientBoostingRegressor(learning_rate=learning_rate, n_estimators=n_estimators, max_depth=max_depth)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print('MSE:', mse)\n",
    "print('R^2:', r2)\n",
    "\n",
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "Ans. In Gradient Boosting, a weak learner is a model that performs only slightly better than random guessing.Typically, a weak learner\n",
    "is a decision tree with a small number of nodes (i.e., low depth). The idea is to use many  weak learners together to create a strong ensemble model.\n",
    "                    \n",
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "Ans. The intuition behind Gradient Boosting is to iteratively add weak learners to the model, with each learner correcting\n",
    "the errors made by the previous learners. The algorithm starts with a simple model (e.g., a decision tree with one node), and then adds\n",
    "more complex models to correct the errors of the simple model. The algorithm uses a gradient descent optimization approach to find the best\n",
    "parameters for each model. The final model is an ensemble of many weak learners, each of which is focused on a specific aspect \n",
    "of the data. The result is a highly accurate model that can handle complex non-linear relationships between the input and output variables.\n",
    "\n",
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "Ans. Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner. At each iteration, a weak learner\n",
    "is added to the ensemble and trained to correct the errors of the previous weak learners. The algorithm starts with a simple model\n",
    "(e.g., a decision tree with one node) and then adds more complex models to correct the errors of the previous models. Each weak learner\n",
    "is trained on the residual errors of the previous model, i.e., the difference between the true output and the output predicted\n",
    "by the previous model. This process continues until the desired number of weak learners has been added to the ensemble or until \n",
    "the error can no longer be reduced.\n",
    "\n",
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "Ans. The mathematical intuition behind Gradient Boosting algorithm can be described in the following steps:\n",
    "\n",
    "The algorithm starts by fitting a simple model (e.g., a decision tree with one node) to the training data. The output of this model\n",
    "is denoted by F_0(x), where x is the input vector.\n",
    "\n",
    "At each iteration m, the algorithm trains a weak learner h_m(x) to predict the residual errors of the previous model, i.e.,\n",
    "the difference between the true output y and the output predicted by the previous model, F_{m-1}(x). The output of the weak learner\n",
    "is denoted by h_m(x).\n",
    "\n",
    "The weak learner is trained to minimize a loss function L(y, F_{m-1}(x) + h_m(x)), where y is the true output and L\n",
    "is a differentiable loss function. The loss function measures the error between the true output and the output predicted by the current model.\n",
    "\n",
    "The weak learner is trained using gradient descent optimization to find the parameters that minimize the loss function.\n",
    "The gradient of the loss function with respect to the parameters is computed using the chain rule.\n",
    "\n",
    "The output of the current model is updated by adding the output of the weak learner multiplied by a small learning rate, i.e., \n",
    "F_m(x) = F_{m-1}(x) + v h_m(x), where v is the learning rate.\n",
    "\n",
    "The algorithm continues to add weak learners to the model until the desired number of weak learners has been added or\n",
    "until the error can no longer be reduced.\n",
    "\n",
    "The final model is an ensemble of many weak learners, each of which is focused on a specific aspect of the data. The output of the \n",
    "final model is given by F(x) = F_0(x) + v sum_{m=1}^M h_m(x), where M is the number of weak learners and v is the learning rate.s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

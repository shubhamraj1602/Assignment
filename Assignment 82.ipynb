{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0327e854-e8d7-4c62-bcc3-1cd0bd7885e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is anomaly detection and what is its purpose?\n",
    "Ans. Anomaly detection refers to the process of identifying patterns or instances that deviate significantly from\n",
    "the norm or expected behavior in a dataset. The purpose of anomaly detection is to detect rare events, outliers, or\n",
    "anomalies that do not conform to the usual patterns or behaviors in order to flag them for further investigation. Anomalies\n",
    "can be indicative of unusual activities, errors, fraud, cybersecurity threats, or other significant events that require attention.\n",
    "\n",
    "Q2. What are the key challenges in anomaly detection?\n",
    "Ans. Key challenges in anomaly detection include:\n",
    "\n",
    "Lack of labeled training data: Anomalies are typically rare, making it challenging to have a sufficient number of labeled \n",
    "instances for supervised learning approaches.\n",
    "Class imbalance: Anomalies are often heavily outnumbered by normal instances, resulting in imbalanced datasets and potential\n",
    "bias towards the majority class.\n",
    "Evolving patterns: Anomalies may change over time, requiring the anomaly detection system to adapt and detect new types of anomalies.\n",
    "Noise and variability: Data may contain noise or variations that make it difficult to distinguish between normal and anomalous patterns.\n",
    "Interpretability: Understanding the reasons behind detected anomalies and explaining them to stakeholders can be complex, especially\n",
    "with complex algorithms.\n",
    "\n",
    "Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?\n",
    "Ans. Unsupervised anomaly detection: It involves detecting anomalies in an unlabeled dataset, where the algorithm learns\n",
    "the normal patterns and flags instances that deviate significantly from them. It does not require prior knowledge or labeled\n",
    "instances of anomalies. Unsupervised methods include statistical approaches, clustering-based methods, density estimation, \n",
    "and distance-based techniques.\n",
    "\n",
    "Supervised anomaly detection: It requires labeled instances of anomalies during the training phase. The algorithm learns the \n",
    "patterns of both normal and anomalous instances to classify future instances. It leverages the knowledge of anomalies to guide \n",
    "the detection process. Supervised methods include classification algorithms such as decision trees, support vector machines,\n",
    "or neural networks.\n",
    "\n",
    "Q4. What are the main categories of anomaly detection algorithms?\n",
    "Ans. The main categories of anomaly detection algorithms are as follows:\n",
    "\n",
    "Statistical Methods: These algorithms assume that the data follows a specific statistical distribution, such as Gaussian\n",
    "distribution, and identify anomalies based on deviations from the expected patterns.\n",
    "\n",
    "Machine Learning Methods: These algorithms utilize various machine learning techniques to learn patterns from the data\n",
    "and identify anomalies based on deviations from the learned patterns. Examples include clustering-based methods, support\n",
    "vector machines (SVM), and decision trees.\n",
    "\n",
    "Information Theory Methods: These algorithms utilize information theory concepts, such as entropy and mutual information,\n",
    "to detect anomalies by measuring the unexpectedness or unpredictability of data points.\n",
    "\n",
    "Distance-Based Methods: These algorithms measure the distance or dissimilarity between data points and identify anomalies\n",
    "based on their distance from the majority of the data points.\n",
    "\n",
    "Density-Based Methods: These algorithms estimate the density of data points and identify anomalies as points with low density \n",
    "compared to their neighbors.\n",
    "\n",
    "Ensemble Methods: These algorithms combine multiple anomaly detection algorithms or models to improve detection accuracy and robustness.\n",
    "\n",
    "Deep Learning Methods: These algorithms leverage deep neural networks and learn complex representations of the data to detect\n",
    "anomalies. Autoencoders and generative adversarial networks (GANs) are commonly used in deep learning-based anomaly detection.\n",
    "\n",
    "Q5. What are the main assumptions made by distance-based anomaly detection methods?\n",
    "Ans. The main assumptions made by distance-based anomaly detection methods are:\n",
    "\n",
    "Normal data points are close to each other in the feature space, forming dense regions, while anomalies are far \n",
    "from the normal data points.\n",
    "\n",
    "The distance metric used is appropriate for measuring the dissimilarity between data points. Common distance metrics\n",
    "include Euclidean distance, Mahalanobis distance, and cosine similarity.\n",
    "\n",
    "The distribution of normal data points is relatively uniform or follows a specific pattern that can be captured by the \n",
    "distance metric. Anomalies are expected to deviate significantly from this distribution.\n",
    "\n",
    "The number of anomalies is relatively small compared to the size of the dataset, assuming that anomalies are rare occurrences.\n",
    "\n",
    "Q6. How does the LOF algorithm compute anomaly scores?\n",
    "Ans. The LOF (Local Outlier Factor) algorithm computes anomaly scores as follows:\n",
    "\n",
    "For each data point, the algorithm measures the local density of its neighbors. The density is typically calculated using a \n",
    "distance-based measure, such as the number of neighbors within a specified radius or the average distance to the k nearest neighbors.\n",
    "\n",
    "The local reachability density (LRD) of a data point is computed by comparing its local density with the densities of it neighbors.\n",
    "The LRD provides a measure of how isolated a data point is compared to its neighbors.\n",
    "\n",
    "The local outlier factor (LOF) of a data point is calculated by comparing its LRD with the LRDs of its neighbors. The LOF quantifies\n",
    "the extent to which a data point deviates from its local neighborhood in terms of density. A higher LOF indicates a higher likelihood\n",
    "of being an anomaly.\n",
    "\n",
    "Anomaly scores are assigned to each data point based on their LOF values. Higher LOF values indicate a higher degree of anomaly.\n",
    "\n",
    "By computing the LOF for each data point, the algorithm can identify anomalies as data points with significantly higher LOF\n",
    "values compared to the majority of the data.\n",
    "\n",
    "Q7. What are the key parameters of the Isolation Forest algorithm?\n",
    "Ans. The key parameters of the Isolation Forest algorithm are:\n",
    "\n",
    "n_estimators: This parameter specifies the number of isolation trees to be created. An isolation tree is a binary tree\n",
    "that splits the data points randomly. Increasing the number of trees can lead to better performance but also increases computational cost.\n",
    "\n",
    "max_samples: It determines the number of samples drawn from the dataset to create each isolation tree. A smaller value can \n",
    "increase the randomness and diversity of the trees, but too small of a value can result in poor performance.\n",
    "\n",
    "contamination: It represents the expected proportion of anomalies in the dataset. This parameter helps in determining the \n",
    "threshold for classifying data points as anomalies. The value should be set based on prior knowledge or estimation of the dataset.\n",
    "\n",
    "max_features: It specifies the maximum number of features to consider when splitting a node in an isolation tree. Setting \n",
    "this value to a smaller number can increase the randomness and diversity of the trees.\n",
    "\n",
    "These are the main parameters of the Isolation Forest algorithm, and they can be adjusted to achieve the desired balance between\n",
    "detection accuracy and computational efficiency.\n",
    "\n",
    "Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?\n",
    "Ans.  If a data point has only 2 neighbors of the same class within a radius of 0.5, the anomaly score using KNN with K=10\n",
    "would depend on the distances to its 10 nearest neighbors. If the distances to its 10 nearest neighbors are all larger than 0.5,\n",
    "it would have a higher anomaly score because it doesn't have enough neighbors of the same class within the given radius. On the\n",
    "other hand, if some of the distances to its 10 nearest neighbors are smaller than 0.5, it would have a lower anomaly score as \n",
    "it has nearby neighbors of the same class. The anomaly score calculation in KNN depends on the distances and distribution of\n",
    "the data points, so a specific score cannot be determined without considering the distances to the nearest neighbors.\n",
    "\n",
    "Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is theanomaly score\n",
    "for a data point that has an average path length of 5.0 compared to the average path length of the trees?\n",
    "\n",
    "Ans. The anomaly score in the Isolation Forest algorithm is inversely related to the average path length of a data point \n",
    "compared to the average path length of the trees. The lower the average path length, the higher the anomaly score. However,\n",
    "it is important to note that the anomaly score is not directly computed using the average path length alone. Instead, the\n",
    "anomaly score is determined by comparing the path lengths of a data point to an average score derived from the training data.\n",
    "\n",
    "To calculate the anomaly score for a data point with an average path length of 5.0 compared to the average path length of\n",
    "the trees in your specific case (100 trees and a dataset of 3000 data points), you would need additional information:\n",
    "\n",
    "The average path length of the trees: This value would need to be provided or computed from the training data using the Isolation\n",
    "Forest algorithm.\n",
    "\n",
    "The range of average path lengths observed in the training data: This information is important to determine the normalization \n",
    "factor for the anomaly score calculation.\n",
    "\n",
    "Once you have these values, you can compute the anomaly score by comparing the data point's average path length to the average\n",
    "path length of the trees, considering the range of average path lengths observed in the training data. A lower average path \n",
    "length compared to the trees' average would result in a higher anomaly score, while a higher average path length would result\n",
    "in a lower anomaly score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
